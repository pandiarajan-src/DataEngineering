{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the basics of creating a Python script, and then create a Python file for the script to run it on the terminal. In this practice notebook, you'll create the building blocks for a script that finds large files on the filesytem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the logic right \n",
    "Start by defining some of the requirements of the script. In this case, we need to:\n",
    "- _Walk_ the filesystem looking at files, directories and sub-directories\n",
    "- Capture file information: is it a file? a directory? what size? what path?\n",
    "- Store that information in a suitable data structure\n",
    "- Report the sorted data what are the largest files by looking at the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass: 0\n",
      "root: .\n",
      "dirs: ['sample_data', 'scripts']\n",
      "files: ['csv2json_excercise.ipynb', 'querying-databases.ipynb', 'large-files.ipynb', 'sqlite-operations.ipynb', 'looping-data-structures.ipynb', 'serializing-json.ipynb']\n",
      "File found: ./csv2json_excercise.ipynb\n",
      "File found: ./querying-databases.ipynb\n",
      "File found: ./large-files.ipynb\n",
      "File found: ./sqlite-operations.ipynb\n",
      "File found: ./looping-data-structures.ipynb\n",
      "File found: ./serializing-json.ipynb\n",
      "pass: 1\n",
      "root: ./sample_data\n",
      "dirs: []\n",
      "files: ['wine-ratings-small.csv', 'wine-ratings.csv', 'wine-ratings.json']\n",
      "File found: ./sample_data/wine-ratings-small.csv\n",
      "File found: ./sample_data/wine-ratings.csv\n",
      "File found: ./sample_data/wine-ratings.json\n",
      "pass: 2\n",
      "root: ./scripts\n",
      "dirs: []\n",
      "files: ['generate_large_files.py', 'generate_sql.py']\n",
      "File found: ./scripts/generate_large_files.py\n",
      "File found: ./scripts/generate_sql.py\n"
     ]
    }
   ],
   "source": [
    "# Update the loop so that it shows the absolute path of a file ignoring directories which we aren't going to track\n",
    "COUNT = 0\n",
    "for root, directories, files in os.walk('.'):\n",
    "    print(f\"pass: {COUNT}\")\n",
    "    COUNT += 1\n",
    "    print(f\"root: {root}\")\n",
    "    print(f\"dirs: {directories}\")\n",
    "    print(f\"files: {files}\")\n",
    "    for _file in files:\n",
    "        full_path = os.path.join(root, _file)\n",
    "        print(f\"File found: {full_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a few objectives completed:\n",
    "- Files are detected\n",
    "- Full paths are being collected\n",
    "\n",
    "Next, we need to find size information. Python uses bytes by default for size, so in addition to capturing the size, we'll need to find a way to change bytes to megabytes or gigabytes to make it easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 105196b - File: ./csv2json_excercise.ipynb\n",
      "Size: 16667b - File: ./querying-databases.ipynb\n",
      "Size: 8113b - File: ./large-files.ipynb\n",
      "Size: 4447b - File: ./sqlite-operations.ipynb\n",
      "Size: 24650b - File: ./looping-data-structures.ipynb\n",
      "Size: 8833b - File: ./serializing-json.ipynb\n",
      "Size: 314894b - File: ./sample_data/wine-ratings-small.csv\n",
      "Size: 13518834b - File: ./sample_data/wine-ratings.csv\n",
      "Size: 355744b - File: ./sample_data/wine-ratings.json\n",
      "Size: 677b - File: ./scripts/generate_large_files.py\n",
      "Size: 549b - File: ./scripts/generate_sql.py\n"
     ]
    }
   ],
   "source": [
    "# Update the loop to include the file size\n",
    "for root, directories, files in os.walk('.'):\n",
    "    for _file in files:\n",
    "        full_path = os.path.join(root, _file)\n",
    "        size = os.path.getsize(full_path)\n",
    "        print(f\"Size: {size}b - File: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"./csv2json_excercise.ipynb\": 105196, \"./querying-databases.ipynb\": 16667, \"./large-files.ipynb\": 8323, \"./sqlite-operations.ipynb\": 4447, \"./looping-data-structures.ipynb\": 24650, \"./serializing-json.ipynb\": 8833, \"./sample_data/wine-ratings-small.csv\": 314894, \"./sample_data/wine-ratings.csv\": 13518834, \"./sample_data/wine-ratings.json\": 355744, \"./scripts/generate_large_files.py\": 677, \"./scripts/generate_sql.py\": 549}\n"
     ]
    }
   ],
   "source": [
    "# Persist the data into a dictionary. Since file paths are unique you can use those as dictionary keys\n",
    "file_metadata = {}\n",
    "for root, directories, files in os.walk('.'):\n",
    "    for _file in files:\n",
    "        full_path = os.path.join(root, _file)\n",
    "        size = os.path.getsize(full_path)\n",
    "        file_metadata[full_path] = size\n",
    "print(file_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now that the metadata is captured and stored in a suitable data structure like a dictionary, report back the results with only the four largest files. Try using other quantities to report on, like the 10 largest files instead of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 13518834 Path: ./sample_data/wine-ratings.csv\n",
      "Size: 355744 Path: ./sample_data/wine-ratings.json\n",
      "Size: 314894 Path: ./sample_data/wine-ratings-small.csv\n",
      "Size: 105196 Path: ./csv2json_excercise.ipynb\n",
      "Size: 24650 Path: ./looping-data-structures.ipynb\n",
      "Size: 16667 Path: ./querying-databases.ipynb\n",
      "Size: 8833 Path: ./serializing-json.ipynb\n",
      "Size: 8323 Path: ./large-files.ipynb\n",
      "Size: 4447 Path: ./sqlite-operations.ipynb\n",
      "Size: 677 Path: ./scripts/generate_large_files.py\n",
      "Size: 549 Path: ./scripts/generate_sql.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for path, size in sorted(file_metadata.items(), key=lambda x:x[1], reverse=True):\n",
    "    print(f\"Size: {size} Path: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot happening in the previous cell. `sorted()` is a built-in function that can sort iterables like Python dictionaries. In this case, we need to sort by the _value_. This is done using the `key` parameter which accepts a `lambda`.\n",
    "`lambda` allows to represent a function in a single line without defining it. That `lambda` expression is the same as defining a function like:\n",
    "\n",
    "```python\n",
    "def by_value(x):\n",
    "    return x[1]\n",
    "```\n",
    "\n",
    "`x` represents two items, the path and the size. The function is returning only the size because that is what we want to sort with. Try changing the `lambda` expression to use `x[0]` instead and see what happens.\n",
    "\n",
    "**Exercise:** Try using a function instead of a `lambda` function and achieve the same result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
